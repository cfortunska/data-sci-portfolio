{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40de9fbc-9db7-40fd-9dfb-e143023335fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>medium</th>\n",
       "      <th>easting_wgs84</th>\n",
       "      <th>northing_wgs84</th>\n",
       "      <th>date_assay</th>\n",
       "      <th>company</th>\n",
       "      <th>structure</th>\n",
       "      <th>lithology</th>\n",
       "      <th>Co_ppm</th>\n",
       "      <th>Cu_ppm</th>\n",
       "      <th>Ni_ppm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9175100</td>\n",
       "      <td>rock</td>\n",
       "      <td>-93.16750</td>\n",
       "      <td>48.60222</td>\n",
       "      <td>11/14/00</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>metasediment</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9175101</td>\n",
       "      <td>rock</td>\n",
       "      <td>-93.16750</td>\n",
       "      <td>48.60222</td>\n",
       "      <td>11/14/00</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9175102</td>\n",
       "      <td>rock</td>\n",
       "      <td>-93.11861</td>\n",
       "      <td>48.60639</td>\n",
       "      <td>11/14/00</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9175103</td>\n",
       "      <td>rock</td>\n",
       "      <td>-93.11861</td>\n",
       "      <td>48.60639</td>\n",
       "      <td>11/14/00</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9175104</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>92.70556</td>\n",
       "      <td>-48.51444</td>\n",
       "      <td>11/14/00</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>metasediment</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample        medium  easting_wgs84  northing_wgs84 date_assay  \\\n",
       "0  9175100          rock      -93.16750        48.60222   11/14/00   \n",
       "1  9175101          rock      -93.16750        48.60222   11/14/00   \n",
       "2  9175102          rock      -93.11861        48.60639   11/14/00   \n",
       "3  9175103          rock      -93.11861        48.60639   11/14/00   \n",
       "4  9175104  rock_outcrop       92.70556       -48.51444   11/14/00   \n",
       "\n",
       "                           company structure     lithology  Co_ppm  Cu_ppm  \\\n",
       "0  united_states_geological_survey       NaN  metasediment    21.0    36.0   \n",
       "1  united_states_geological_survey       NaN           NaN    29.0   198.0   \n",
       "2  united_states_geological_survey       NaN           NaN    11.0    91.0   \n",
       "3  united_states_geological_survey       NaN           NaN    17.0    63.0   \n",
       "4  united_states_geological_survey       NaN  metasediment    18.0    28.0   \n",
       "\n",
       "  Ni_ppm  \n",
       "0     75  \n",
       "1     44  \n",
       "2     18  \n",
       "3     32  \n",
       "4     77  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing packages and dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "df=pd.read_csv(r'/Users/cecylia/Desktop/Job/data sci/portfolio/Mining/MN_mining_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5c42b-6575-43ee-869e-46e091b75532",
   "metadata": {},
   "source": [
    "Taking a first look at the data I already see some issues just looking at the first five rows, such as missing values. Lets take a closer glace by looking at the info to see what kind of data types we are dealing with and how many values may be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b6320b-7470-4c8d-916b-cba25092eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 305 entries, 0 to 304\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   sample          305 non-null    int64  \n",
      " 1   medium          305 non-null    object \n",
      " 2   easting_wgs84   305 non-null    float64\n",
      " 3   northing_wgs84  304 non-null    float64\n",
      " 4   date_assay      305 non-null    object \n",
      " 5   company         305 non-null    object \n",
      " 6   structure       110 non-null    object \n",
      " 7   lithology       289 non-null    object \n",
      " 8   Co_ppm          302 non-null    float64\n",
      " 9   Cu_ppm          242 non-null    float64\n",
      " 10  Ni_ppm          305 non-null    object \n",
      "dtypes: float64(4), int64(1), object(6)\n",
      "memory usage: 26.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f6e5a0-9dc9-4f3d-9a65-6fe38cda1936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>easting_wgs84</th>\n",
       "      <th>northing_wgs84</th>\n",
       "      <th>Co_ppm</th>\n",
       "      <th>Cu_ppm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.050000e+02</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>242.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.375963e+06</td>\n",
       "      <td>-87.463830</td>\n",
       "      <td>44.920253</td>\n",
       "      <td>46.256391</td>\n",
       "      <td>117.286674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.101207e+05</td>\n",
       "      <td>27.064142</td>\n",
       "      <td>18.343611</td>\n",
       "      <td>70.558872</td>\n",
       "      <td>9115.185410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.175100e+06</td>\n",
       "      <td>-93.350000</td>\n",
       "      <td>-93.200000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-99999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.278678e+06</td>\n",
       "      <td>-92.406940</td>\n",
       "      <td>47.587293</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.334338e+06</td>\n",
       "      <td>-91.925830</td>\n",
       "      <td>47.816670</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.497293e+06</td>\n",
       "      <td>-91.116670</td>\n",
       "      <td>48.143752</td>\n",
       "      <td>46.150000</td>\n",
       "      <td>96.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.500207e+06</td>\n",
       "      <td>92.936110</td>\n",
       "      <td>48.666670</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sample  easting_wgs84  northing_wgs84      Co_ppm        Cu_ppm\n",
       "count  3.050000e+02     305.000000      304.000000  302.000000    242.000000\n",
       "mean   9.375963e+06     -87.463830       44.920253   46.256391    117.286674\n",
       "std    1.101207e+05      27.064142       18.343611   70.558872   9115.185410\n",
       "min    9.175100e+06     -93.350000      -93.200000   -0.100000 -99999.000000\n",
       "25%    9.278678e+06     -92.406940       47.587293    7.500000     12.500000\n",
       "50%    9.334338e+06     -91.925830       47.816670   24.200000     38.000000\n",
       "75%    9.497293e+06     -91.116670       48.143752   46.150000     96.750000\n",
       "max    9.500207e+06      92.936110       48.666670  768.000000  99999.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2fa2a7-b757-4808-9b46-625ff6f977a7",
   "metadata": {},
   "source": [
    "It looks like there are quite a bit of null values to deal with-luckily most of the missing values are on columns that we don't need to answer the final question. There are some columns that should be floats/integers, but are object. After doing summary stats on floats/integer columns I see some lat and longs that dont make sense such as positive 92.94 for long and -93 for lat. This dataset is for MN so this is not possible. We will need to fix this as well. There are also some neagtive Co and Cu values that need to be fixed, as well as some 99999's which indicates NaN most likely. After taking a look at the null value sum of each column I will start cleaning the problematic columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee6d43d-f4e9-4369-a1e6-b5ecf128fab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "structure         195\n",
       "Cu_ppm             63\n",
       "lithology          16\n",
       "Co_ppm              3\n",
       "northing_wgs84      1\n",
       "sample              0\n",
       "medium              0\n",
       "easting_wgs84       0\n",
       "date_assay          0\n",
       "company             0\n",
       "Ni_ppm              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f1d7b-0b28-4386-9aa8-69a044f41af7",
   "metadata": {},
   "source": [
    "I am going to create a function that looks for unique values for each column to see how many unique values there are in a column. This is great for columns that shouldn't have a lot of unique values (such as lithology) and columns that should have a unique value for each entry (such as sample). This will make spotting small errors a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898d03af-ad02-40cb-b019-8f29ada3f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function\n",
    "def count_unique(df):\n",
    "    \"\"\"Returns a single value of how many unique values there are in the dataframe column\"\"\"\n",
    "    count=df.nunique()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b1cfa-cc88-4188-b646-764a5f887100",
   "metadata": {},
   "source": [
    "Let's start with the most important columns we need to answer our question. I am going to start with the lithology column and apply our created function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef0ca24-2cf2-4dba-a17a-636e815e2a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many different lithologies in lithology column\n",
    "count_unique(df['lithology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564ce7c-4248-43a4-9f1d-d71c7e0bf227",
   "metadata": {},
   "source": [
    "That's a lot of lithologies! Let's see what's going on and also explore the 16 nan values as well (indicated by df.info(). Let's create a second function that lists all unique values. I imagine we will need to do this with other columns so creating a funtion will make the task faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d4b8e4-6fe3-4d5c-a8f0-2f351c098081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function\n",
    "def list_unique(df):\n",
    "    \"\"\"Returns all unique values in the dataframe column\"\"\"\n",
    "    values=df.unique()\n",
    "    values.sort()\n",
    "    for v in values:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27100979-ec89-40a3-93e6-70f377be437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basalt\n",
      "CHERT\n",
      "DACITE\n",
      "GABBRO\n",
      "GRAYWACKE\n",
      "andesite\n",
      "argillite\n",
      "basalt\n",
      "basalt.\n",
      "basaltt\n",
      "bassalt\n",
      "dacite\n",
      "diorite\n",
      "felsic\n",
      "gabbro\n",
      "gabro\n",
      "granite\n",
      "granodiorite\n",
      "greenstone\n",
      "hornfels\n",
      "meta-andesite\n",
      "meta-graywacke\n",
      "metafelsite\n",
      "metagraywacke\n",
      "metasediment\n",
      "metasiltstone\n",
      "monzonite\n",
      "nan\n",
      "none\n",
      "pegmatite\n",
      "rhyolite\n",
      "sandstone\n",
      "schist\n",
      "shale\n",
      "siliciclastic\n",
      "tuff\n"
     ]
    }
   ],
   "source": [
    "#list of all lithologies in lithology column\n",
    "df['lithology']=df['lithology'].astype('str')\n",
    "list_unique(df['lithology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b573b6f-1ffc-489d-aceb-21e1b7ab8761",
   "metadata": {},
   "source": [
    "Looks like there are a lot of data entry mistakes such as uppercase/lowercase repeats and spelling mistakes. Also there are a few 'none'. Let's change those to unknown. Let's also change the rest of the nan's to unknown as well for now. We can have a look at the rock formations spatially later to decide if some unknowns can be changed based on location-however this is often not a good idea-especially in complex geological areas. I will also take a look at the range of Co/Ni of these unknowns a little later to decide if the points are even important to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6df2e3-fa8d-4425-8631-cbcc012fb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up the lithology column by making everything lowercase, filling in unknown, and correcting spelling mistakes\n",
    "df['lithology'] = df['lithology'].str.lower()\n",
    "df['lithology'] = df['lithology'].replace('none', 'unknown', regex=False)\n",
    "df['lithology']=df.lithology.replace(dict.fromkeys(['bassalt','basaltt','basalt.'], 'basalt'))\n",
    "df['lithology'] = df['lithology'].replace('gabro', 'gabbro', regex=False)\n",
    "df['lithology'] = df['lithology'].replace('-', '', regex=False)\n",
    "df['lithology']=df['lithology'].replace('nan', 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18aee73d-34e2-4531-8077-2c2fc7068847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andesite\n",
      "argillite\n",
      "basalt\n",
      "chert\n",
      "dacite\n",
      "diorite\n",
      "felsic\n",
      "gabbro\n",
      "granite\n",
      "granodiorite\n",
      "graywacke\n",
      "greenstone\n",
      "hornfels\n",
      "meta-andesite\n",
      "meta-graywacke\n",
      "metafelsite\n",
      "metagraywacke\n",
      "metasediment\n",
      "metasiltstone\n",
      "monzonite\n",
      "pegmatite\n",
      "rhyolite\n",
      "sandstone\n",
      "schist\n",
      "shale\n",
      "siliciclastic\n",
      "tuff\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "list_unique(df['lithology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7156e-30b9-4cee-9e5f-9eb8c7040e85",
   "metadata": {},
   "source": [
    "Looks good for now. Let's move on and do something similar to the other columns before dealing with unknown a bit more in depth. Those negative ppm's in Co need to be fixed. I am going to switch them to positive. There are also a few blanks, however we can take a look at those 3 NaN's a little later to decide what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecd8788c-02f6-4154-a2e2-b6431a01f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing so all values are positive\n",
    "df['Co_ppm'] = df['Co_ppm'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2afc9c-30da-4808-a6e2-68e66376bc45",
   "metadata": {},
   "source": [
    "Unfortunetly summary stats on Ni could not be done since the datatype is not an integer or float-which of course is also an indication that something is fishy about the data. Let's apply out list unique function on this as well since this is a realively small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43957231-8bb2-4af1-8671-8f44a8cc9f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.25\n",
      "1.75\n",
      "100\n",
      "1011\n",
      "105\n",
      "106.5\n",
      "11\n",
      "11.5\n",
      "110\n",
      "111\n",
      "12\n",
      "12.5\n",
      "120\n",
      "1200\n",
      "124\n",
      "125\n",
      "126\n",
      "130\n",
      "132\n",
      "134\n",
      "138\n",
      "139\n",
      "14\n",
      "140\n",
      "145\n",
      "15\n",
      "150\n",
      "16\n",
      "162\n",
      "165\n",
      "169\n",
      "17.5\n",
      "172\n",
      "18\n",
      "180\n",
      "184.5\n",
      "188\n",
      "19\n",
      "190\n",
      "195\n",
      "196\n",
      "2\n",
      "2.5\n",
      "20\n",
      "201\n",
      "214.5\n",
      "22\n",
      "23\n",
      "232\n",
      "240\n",
      "242\n",
      "247.5\n",
      "25\n",
      "252\n",
      "254\n",
      "26\n",
      "260\n",
      "270\n",
      "277\n",
      "29\n",
      "3.75\n",
      "3.9\n",
      "30\n",
      "300\n",
      "32\n",
      "34\n",
      "35\n",
      "351\n",
      "36\n",
      "364.5\n",
      "37.5\n",
      "381\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "430\n",
      "44\n",
      "440\n",
      "45\n",
      "450\n",
      "46\n",
      "470\n",
      "48\n",
      "5\n",
      "5.5\n",
      "5.7\n",
      "50\n",
      "500\n",
      "501\n",
      "517.5\n",
      "52\n",
      "526.5\n",
      "53\n",
      "54\n",
      "55\n",
      "58\n",
      "6\n",
      "6.1\n",
      "61\n",
      "618\n",
      "619.5\n",
      "62\n",
      "63\n",
      "64\n",
      "640\n",
      "65\n",
      "66\n",
      "67\n",
      "67.5\n",
      "68\n",
      "7\n",
      "7.5\n",
      "70\n",
      "71\n",
      "73\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "8\n",
      "801\n",
      "81\n",
      "82\n",
      "83\n",
      "85\n",
      "86\n",
      "87\n",
      "88.5\n",
      "9\n",
      "90\n",
      "91\n",
      "912\n",
      "93\n",
      "934.5\n",
      "96\n",
      "98\n",
      "<1.5\n"
     ]
    }
   ],
   "source": [
    "list_unique(df['Ni_ppm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f53322-60ae-4084-af20-0c804f6b9c92",
   "metadata": {},
   "source": [
    "A few issues here. Looking at this closer it looks like the <1.5 is the issue and will just be changed to 1.4 so that we can perform analysis on this column. Also the negative numbers will be changed to positive numbers, just like the Co_ppm column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89950cbe-8868-4879-b56f-485905d9914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing values and removing negatives\n",
    "df['Ni_ppm'] = df['Ni_ppm'].str.replace(\"<1.5\", \"1.4\", regex=False)\n",
    "df['Ni_ppm']=df['Ni_ppm'].astype('float')\n",
    "df['Ni_ppm']=df['Ni_ppm'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335a52c-f8f4-409d-b3ff-a0d2029fcfac",
   "metadata": {},
   "source": [
    "Let's move on to the lat column. There are a lot of issues with this column given the summary stats that were performed earlier. 1)A few lats in the -90s which does not make sense. I have a hunch they are flipped with the longitude column. We will validate this in a little bit. 2)A few negative lats in the 40s which also doesn't make sense. Those - signs will be dropped. I also have a hunch the - sign is missing in the longitude column. We will verify that as well in a little bit. 3)There is a missing value here and no missing values in the longitude column. This type of error can be a little tricky. We will take a closer look at the row in which that value is missing to determine what we can do (drop it because it's not needed or make an assumption based on other columns). Let's take a closer look at this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7301aed8-4c68-4da5-9605-dc9f78db9ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>medium</th>\n",
       "      <th>easting_wgs84</th>\n",
       "      <th>northing_wgs84</th>\n",
       "      <th>date_assay</th>\n",
       "      <th>company</th>\n",
       "      <th>structure</th>\n",
       "      <th>lithology</th>\n",
       "      <th>Co_ppm</th>\n",
       "      <th>Cu_ppm</th>\n",
       "      <th>Ni_ppm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>9500207</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>-91.83333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/8/91</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>basalt</td>\n",
       "      <td>43.8</td>\n",
       "      <td>96.0</td>\n",
       "      <td>254.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample        medium  easting_wgs84  northing_wgs84 date_assay  \\\n",
       "299  9500207  rock_outcrop      -91.83333             NaN     8/8/91   \n",
       "\n",
       "                             company  structure lithology  Co_ppm  Cu_ppm  \\\n",
       "299  united_states_geological_survey  lava flow    basalt    43.8    96.0   \n",
       "\n",
       "     Ni_ppm  \n",
       "299   254.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.northing_wgs84.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313515f3-fb86-40c1-a5f1-47560ac9cd12",
   "metadata": {},
   "source": [
    "Unfortunetly, this is a basalt entry which makes me hestitant to drop the row. The Ni/Co ppm does look to be pretty low compared to other values I have seen in the dataset. Ultimately, I am deciding to drop this one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5c7625e-f986-44e7-9f64-a7d6601c96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the row\n",
    "df=df.drop(299)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae779d3-cd02-4a72-8c76-0334c23fce48",
   "metadata": {},
   "source": [
    "Let's deal with the lat and longs and see what may be flipped due to data entry errors. First, I will double check to make sure these value misplacement/mistakes aligns row to row. Next,  I will change all numbers to positive in the easting column so that I can do a quick swap on all numbers less than 49 in the easting column. Then covert both columns to their correct - and + values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53604bc-6809-4ea7-a80f-57fd41ef1715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>medium</th>\n",
       "      <th>easting_wgs84</th>\n",
       "      <th>northing_wgs84</th>\n",
       "      <th>date_assay</th>\n",
       "      <th>company</th>\n",
       "      <th>structure</th>\n",
       "      <th>lithology</th>\n",
       "      <th>Co_ppm</th>\n",
       "      <th>Cu_ppm</th>\n",
       "      <th>Ni_ppm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9175104</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>92.70556</td>\n",
       "      <td>-48.51444</td>\n",
       "      <td>11/14/00</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>metasediment</td>\n",
       "      <td>18.00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9177307</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>48.41581</td>\n",
       "      <td>-92.52700</td>\n",
       "      <td>11/6/01</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>pluton; stock</td>\n",
       "      <td>granite</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9262291</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>92.91667</td>\n",
       "      <td>-46.43056</td>\n",
       "      <td>1/11/79</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>schist</td>\n",
       "      <td>33.00</td>\n",
       "      <td>78.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9262294</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>92.93611</td>\n",
       "      <td>-46.33333</td>\n",
       "      <td>1/11/79</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>schist</td>\n",
       "      <td>15.00</td>\n",
       "      <td>3.2</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>9278672</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>48.63333</td>\n",
       "      <td>-93.20000</td>\n",
       "      <td>11/14/83</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>xenolith</td>\n",
       "      <td>diorite</td>\n",
       "      <td>66.00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>9334348</td>\n",
       "      <td>rock</td>\n",
       "      <td>47.61861</td>\n",
       "      <td>-91.93222</td>\n",
       "      <td>1/2/88</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gabbro</td>\n",
       "      <td>22.50</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>9493009</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>92.13333</td>\n",
       "      <td>-47.85000</td>\n",
       "      <td>8/7/85</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>greenstone</td>\n",
       "      <td>0.10</td>\n",
       "      <td>29.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>9494367</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>90.33333</td>\n",
       "      <td>-47.75000</td>\n",
       "      <td>1/8/87</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>rhyolite</td>\n",
       "      <td>0.41</td>\n",
       "      <td>53.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample        medium  easting_wgs84  northing_wgs84 date_assay  \\\n",
       "4    9175104  rock_outcrop       92.70556       -48.51444   11/14/00   \n",
       "21   9177307  rock_outcrop       48.41581       -92.52700    11/6/01   \n",
       "30   9262291  rock_outcrop       92.91667       -46.43056    1/11/79   \n",
       "33   9262294  rock_outcrop       92.93611       -46.33333    1/11/79   \n",
       "70   9278672  rock_outcrop       48.63333       -93.20000   11/14/83   \n",
       "161  9334348          rock       47.61861       -91.93222     1/2/88   \n",
       "185  9493009  rock_outcrop       92.13333       -47.85000     8/7/85   \n",
       "195  9494367  rock_outcrop       90.33333       -47.75000     1/8/87   \n",
       "\n",
       "                             company      structure     lithology  Co_ppm  \\\n",
       "4    united_states_geological_survey            NaN  metasediment   18.00   \n",
       "21   united_states_geological_survey  pluton; stock       granite    3.00   \n",
       "30   united_states_geological_survey            NaN        schist   33.00   \n",
       "33   united_states_geological_survey            NaN        schist   15.00   \n",
       "70   united_states_geological_survey       xenolith       diorite   66.00   \n",
       "161  united_states_geological_survey            NaN        gabbro   22.50   \n",
       "185  united_states_geological_survey            NaN    greenstone    0.10   \n",
       "195  united_states_geological_survey      lava flow      rhyolite    0.41   \n",
       "\n",
       "     Cu_ppm  Ni_ppm  \n",
       "4      28.0    77.0  \n",
       "21      4.0     1.4  \n",
       "30     78.0   100.0  \n",
       "33      3.2    63.0  \n",
       "70     43.0   640.0  \n",
       "161    75.0    75.0  \n",
       "185    29.0    67.0  \n",
       "195    53.0    15.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking out the faulty values to see if they all align-looks like they do.\n",
    "df.loc[df['easting_wgs84']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac1869-841d-486d-8701-1a913f61f130",
   "metadata": {},
   "source": [
    "Just as suspected there are a few lats/longs that belong in the opposite columns due to human entry error. Also a few postive and negative sings that needs to be switched. Lets change all long columns to positive first to make the swap a little easier. Once swapped this will all get changed back to negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e08932ff-e87e-492e-adde-2624fa0876f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing longs to postive\n",
    "df['easting_wgs84']=df['easting_wgs84'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23cc641d-aaee-4b5d-a002-8a54cbd82507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#swapping the values between rows that need to be swapped\n",
    "swap=df['easting_wgs84']<49.0\n",
    "df.loc[swap, ['easting_wgs84','northing_wgs84']] = df.loc[swap, ['northing_wgs84','easting_wgs84']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ffdba4d-f1d9-4098-98df-e05ee81d426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets finish up by putting a negative sign infront of each long\n",
    "df['easting_wgs84']=df['easting_wgs84'].abs()\n",
    "df['easting_wgs84']*=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3caa0038-5808-4deb-80ed-a64dbe628c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    304.000000\n",
       "mean     -91.871750\n",
       "std        0.991634\n",
       "min      -93.350000\n",
       "25%      -92.532750\n",
       "50%      -91.934860\n",
       "75%      -91.116670\n",
       "max      -89.639140\n",
       "Name: easting_wgs84, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets take a look to see if the swap worked out before continue\n",
    "df.easting_wgs84.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e982bec-6e2d-417a-a9c4-99282f77f08b",
   "metadata": {},
   "source": [
    "This look good and makes much more sense. Let's change the lat column to positive do a final verification of the column and move on to other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15f0a903-8312-4104-bb05-7cf6732d61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive sign for each lat\n",
    "df['northing_wgs84']=df['northing_wgs84'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9e311af-d78c-4593-ba5f-1f016368262b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    304.000000\n",
       "mean      47.867896\n",
       "std        0.538143\n",
       "min       46.333330\n",
       "25%       47.599860\n",
       "50%       47.850000\n",
       "75%       48.150000\n",
       "max       48.666670\n",
       "Name: northing_wgs84, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final verification\n",
    "df.northing_wgs84.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e1c06-48cb-47c5-bfac-dcff7c337ea3",
   "metadata": {},
   "source": [
    "Looks good. Moving on to sample column. I want to make sure there are no duplicate entries so let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b33d893d-c2b9-4fec-b182-feea2834bc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>medium</th>\n",
       "      <th>easting_wgs84</th>\n",
       "      <th>northing_wgs84</th>\n",
       "      <th>date_assay</th>\n",
       "      <th>company</th>\n",
       "      <th>structure</th>\n",
       "      <th>lithology</th>\n",
       "      <th>Co_ppm</th>\n",
       "      <th>Cu_ppm</th>\n",
       "      <th>Ni_ppm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>9497291</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>-90.33333</td>\n",
       "      <td>48.00</td>\n",
       "      <td>3/20/89</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>basalt</td>\n",
       "      <td>42.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>9497345</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>-90.41667</td>\n",
       "      <td>47.75</td>\n",
       "      <td>4/5/89</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>basalt</td>\n",
       "      <td>59.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>9497345</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>-90.41667</td>\n",
       "      <td>47.75</td>\n",
       "      <td>4/5/89</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>basalt</td>\n",
       "      <td>36.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>9497345</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>-90.41667</td>\n",
       "      <td>47.75</td>\n",
       "      <td>4/5/89</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>basalt</td>\n",
       "      <td>52.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>9497291</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>-90.33333</td>\n",
       "      <td>48.00</td>\n",
       "      <td>3/20/89</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>basalt</td>\n",
       "      <td>42.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>9497291</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>-90.33333</td>\n",
       "      <td>48.00</td>\n",
       "      <td>3/20/89</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>basalt</td>\n",
       "      <td>42.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>9497291</td>\n",
       "      <td>rock_outcrop</td>\n",
       "      <td>-90.33333</td>\n",
       "      <td>48.00</td>\n",
       "      <td>3/20/89</td>\n",
       "      <td>united_states_geological_survey</td>\n",
       "      <td>lava flow</td>\n",
       "      <td>basalt</td>\n",
       "      <td>42.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample        medium  easting_wgs84  northing_wgs84 date_assay  \\\n",
       "223  9497291  rock_outcrop      -90.33333           48.00    3/20/89   \n",
       "233  9497345  rock_outcrop      -90.41667           47.75     4/5/89   \n",
       "300  9497345  rock_outcrop      -90.41667           47.75     4/5/89   \n",
       "301  9497345  rock_outcrop      -90.41667           47.75     4/5/89   \n",
       "302  9497291  rock_outcrop      -90.33333           48.00    3/20/89   \n",
       "303  9497291  rock_outcrop      -90.33333           48.00    3/20/89   \n",
       "304  9497291  rock_outcrop      -90.33333           48.00    3/20/89   \n",
       "\n",
       "                             company  structure lithology  Co_ppm  Cu_ppm  \\\n",
       "223  united_states_geological_survey  lava flow    basalt    42.0   100.0   \n",
       "233  united_states_geological_survey  lava flow    basalt    59.0    83.0   \n",
       "300  united_states_geological_survey  lava flow    basalt    36.0   110.0   \n",
       "301  united_states_geological_survey  lava flow    basalt    52.0    58.0   \n",
       "302  united_states_geological_survey  lava flow    basalt    42.0   100.0   \n",
       "303  united_states_geological_survey  lava flow    basalt    42.0   100.0   \n",
       "304  united_states_geological_survey  lava flow    basalt    42.0   100.0   \n",
       "\n",
       "     Ni_ppm  \n",
       "223    41.0  \n",
       "233   172.0  \n",
       "300    41.0  \n",
       "301   242.0  \n",
       "302    41.0  \n",
       "303    41.0  \n",
       "304    41.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.duplicated(subset=['sample'], keep=False), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe0f7d-ab39-4a09-8f73-12cfc21a1840",
   "metadata": {},
   "source": [
    "It looks like sample 9497291 got entered 4 times by accident. We can drop those duplicates. However, first we need to deal with sample 9497345 which is a bit confusing. It is in the same exact position, however the co/cu/ni ppm's are different. We will assign two of those samples a new ID number just to be safe instead of dropping it. Let's take a look at all unique values  in the sample column so that we can give it a proper unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa955c23-6585-477d-83ae-9b05ea3d3cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ffedd4-539f-497a-ac0d-4583ac09f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unique(df['sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19f2c2-e2b8-4679-87ae-afa71fb54957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign new ID\n",
    "df.loc[301, 'sample']=9500207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ceba48-5142-4b8a-881a-5da424ee8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning new ID\n",
    "df.loc[300, 'sample']=9500208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c8454-4125-4dff-a2b5-8f3f65ef9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the identical rows now\n",
    "df.drop_duplicates(subset=['sample'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0001c81-ba82-45b6-bf5b-7bbeb5afe030",
   "metadata": {},
   "source": [
    "Just making sure we didn't lose any entries. Looks like three entries in total got dropped which makes sense. Also make sure all duplicates are gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9c5f1-02c3-46a9-95c3-cf4bfb737bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45274529-20b5-4f0d-928a-55bbe031f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unique(df['sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a69c8e-36e6-4c38-9a21-e571218d2db3",
   "metadata": {},
   "source": [
    "Lets move on to the structure column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecacd3c-e2fb-4a2e-bd11-3cf9974757fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['structure']=df['structure'].astype('str')\n",
    "count_unique(df['structure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a7419-2541-4435-a521-fbd582545167",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unique(df['structure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c1cf5-6e62-428c-a8a0-39fe191fd569",
   "metadata": {},
   "source": [
    "This looks pretty good. Let's just put nan as unknown and move on. While we are not concerned with Cu for this assignment I decided to clean it up a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9a231-a1cb-48e7-9e51-dfd4b0a4caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['structure']=df['structure'].replace('nan', 'unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7afcf5d-7552-488c-8c29-175cf0bb30e9",
   "metadata": {},
   "source": [
    "Let's get rid of all negatives in the Cu column, as well as replace '99999' with nan due to error. All nan will remain nan. Normally, I may deal with these nan in a different manner however this column isn't relevant to the current assigment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b454e-1823-4f6a-951c-a700279ea026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cu_ppm']=df['Cu_ppm'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e128dea-333a-4881-8457-3b79bb90767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace 99999 with nan\n",
    "df.Cu_ppm.replace(99999.0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05323358-7410-44aa-b2c0-ae8abff90c38",
   "metadata": {},
   "source": [
    "Moving on to the rest of the columns before I start the visual component of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d05f2d-05df-4e84-80e5-74a9c7655e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unique(df['date_assay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357e270-c742-46be-88d0-86f8094aa08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unique(df['medium'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dac694-9020-4bf7-bf25-d0b2b1afa720",
   "metadata": {},
   "source": [
    "Everything else looks pretty good. Before moving on to answering the question, lets take a final look at the lithology column and decide what to do with all the unknown. First, I want to take a look at some stats. Second, I want to quickly map the data in qgis to see how the points look in relation to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d589d6-3332-452b-b96d-0d78105e252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('lithology')['Co_ppm', 'Ni_ppm'].agg(['mean','min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a7f51-89c4-44ff-8ab3-792f1b71baf8",
   "metadata": {},
   "source": [
    "I can already tell that gabbro may perhaps be a better prospect, however no conclusions can be made without visuals. Looks like the unknown max is at 330 ppm for Co and 110 ppm for Ni. While losing data is not ideal, we are not losing high ppm points compared to others in the dataset. Also, looking at the map in qgis led me to the conclusion that closest distance imputation for unknowns is not a good idea. The geology in northern MN is very complex and erratic. Let's take a look at those three missing Co_ppm values before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a694324-b7b0-4410-beb7-88399fde41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Co_ppm.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644c037-aac8-467f-bfc2-31445614e255",
   "metadata": {},
   "source": [
    "Since these entries are all greenstone let's leave these values as NaN since the task at hand does not concern greenstone lithology. Otherwise, these NaN values would have been imputed. Let's create some geodataframes and do a preliminary plot in python before moving to qgis to create our final map with the raster. I will create two geodataframes-basalt and gabbro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79bedc-7865-40e4-ad5b-53761879937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basalt_gabbro=df[(df['lithology']=='basalt') | (df['lithology']=='gabbro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95069339-99d2-48dc-be52-d79cc5b59538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to geodataframe\n",
    "from pyproj import CRS\n",
    "crs=CRS('EPSG:4326')\n",
    "df['geometry']=df.apply(lambda x: Point((x.easting_wgs84, x.northing_wgs84)), axis=1)\n",
    "df_geo=gpd.GeoDataFrame(df, crs=crs, geometry=df.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24687079-704e-482a-b608-9eda81ee3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating into two seperate geodataframes\n",
    "basalt_geo=df_geo[df_geo['lithology']=='basalt']\n",
    "gabbro_geo=df_geo[df['lithology']=='gabbro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420ccdc-ac1c-4fd7-8624-a20b0ce370c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basalt/gabbro plots\n",
    "fig, axes=plt.subplots(ncols=2,nrows=2, figsize=(20,14))\n",
    "basalt_geo.plot(column='Co_ppm',ax=axes[0,0],legend=True)\n",
    "axes[0,0].set_title('Basalt Co_ppm')\n",
    "basalt_geo.plot(column='Ni_ppm',ax=axes[0,1], legend=True)\n",
    "axes[0,1].set_title('Basalt Ni_ppm')\n",
    "gabbro_geo.plot(column='Co_ppm',ax=axes[1,0], legend=True)\n",
    "axes[1,0].set_title('Gabbro Co_ppm')\n",
    "gabbro_geo.plot(column='Ni_ppm',ax=axes[1,1], legend=True)\n",
    "axes[1,1].set_title('Gabbro Ni_ppm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a885a-a0d6-45b7-b455-b3571257ecba",
   "metadata": {},
   "source": [
    "I have a pretty good idea of the statistical distrubution and what areas are good targets, however lets move this project to Tableau for final analysis and mapping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b35328-2a3d-42c2-81bc-127ceb72a633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
